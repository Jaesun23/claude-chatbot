메시지
Create a Message
Send a structured list of input messages with text and/or image content, and the model will generate the next message in the conversation.

The Messages API can be used for either single queries or stateless multi-turn conversations.

POST
/
v1
/
messages
Headers
anthropic-beta
string[]
Optional header to specify the beta version(s) you want to use.

To use multiple betas, use a comma separated list like beta1,beta2 or specify the header multiple times for each beta.

anthropic-version
string
required
The version of the Anthropic API you want to use.

Read more about versioning and our version history here.

x-api-key
string
required
Your unique API key for authentication.

This key is required in the header of all API requests, to authenticate your account and access Anthropic's services. Get your API key through the Console. Each key is scoped to a Workspace.

Body
application/json
model
string
required
The model that will complete your prompt.

See models for additional details and options.

messages
object[]
required
Input messages.

Our models are trained to operate on alternating user and assistant conversational turns. When creating a new Message, you specify the prior conversational turns with the messages parameter, and the model then generates the next Message in the conversation. Consecutive user or assistant turns in your request will be combined into a single turn.

Each input message must be an object with a role and content. You can specify a single user-role message, or you can include multiple user and assistant messages.

If the final message uses the assistant role, the response content will continue immediately from the content in that message. This can be used to constrain part of the model's response.

Example with a single user message:

[{"role": "user", "content": "Hello, Claude"}]
Example with multiple conversational turns:

[
  {"role": "user", "content": "Hello there."},
  {"role": "assistant", "content": "Hi, I'm Claude. How can I help you?"},
  {"role": "user", "content": "Can you explain LLMs in plain English?"},
]
Example with a partially-filled response from Claude:

[
  {"role": "user", "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"},
  {"role": "assistant", "content": "The best answer is ("},
]
Each input message content may be either a single string or an array of content blocks, where each block has a specific type. Using a string for content is shorthand for an array of one content block of type "text". The following input messages are equivalent:

{"role": "user", "content": "Hello, Claude"}
{"role": "user", "content": [{"type": "text", "text": "Hello, Claude"}]}
Starting with Claude 3 models, you can also send image content blocks:

{"role": "user", "content": [
  {
    "type": "image",
    "source": {
      "type": "base64",
      "media_type": "image/jpeg",
      "data": "/9j/4AAQSkZJRg...",
    }
  },
  {"type": "text", "text": "What is in this image?"}
]}
We currently support the base64 source type for images, and the image/jpeg, image/png, image/gif, and image/webp media types.

See examples for more input examples.

Note that if you want to include a system prompt, you can use the top-level system parameter — there is no "system" role for input messages in the Messages API.


Hide child attributes

messages.role
enum<string>
required
Available options: user, assistant 
messages.content

string
required
max_tokens
integer
required
The maximum number of tokens to generate before stopping.

Note that our models may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.

Different models have different maximum values for this parameter. See models for details.

metadata
object
An object describing metadata about the request.


Hide child attributes

metadata.user_id
string | null
An external identifier for the user who is associated with the request.

This should be a uuid, hash value, or other opaque identifier. Anthropic may use this id to help detect abuse. Do not include any identifying information such as name, email address, or phone number.

stop_sequences
string[]
Custom text sequences that will cause the model to stop generating.

Our models will normally stop when they have naturally completed their turn, which will result in a response stop_reason of "end_turn".

If you want the model to stop generating when it encounters custom strings of text, you can use the stop_sequences parameter. If the model encounters one of the custom sequences, the response stop_reason value will be "stop_sequence" and the response stop_sequence value will contain the matched stop sequence.

stream
boolean
Whether to incrementally stream the response using server-sent events.

See streaming for details.

system

string
System prompt.

A system prompt is a way of providing context and instructions to Claude, such as specifying a particular goal or role. See our guide to system prompts.

temperature
number
Amount of randomness injected into the response.

Defaults to 1.0. Ranges from 0.0 to 1.0. Use temperature closer to 0.0 for analytical / multiple choice, and closer to 1.0 for creative and generative tasks.

Note that even with temperature of 0.0, the results will not be fully deterministic.

tool_choice
object
How the model should use the provided tools. The model can use a specific tool, any available tool, or decide by itself.

Auto
Any
Tool

Hide child attributes

tool_choice.type
enum<string>
required
Available options: auto 
tool_choice.disable_parallel_tool_use
boolean
Whether to disable parallel tool use.

Defaults to false. If set to true, the model will output at most one tool use.

tools
object[]
Definitions of tools that the model may use.

If you include tools in your API request, the model may return tool_use content blocks that represent the model's use of those tools. You can then run those tools using the tool input generated by the model and then optionally return results back to the model using tool_result content blocks.

Each tool definition includes:

name: Name of the tool.
description: Optional, but strongly-recommended description of the tool.
input_schema: JSON schema for the tool input shape that the model will produce in tool_use output content blocks.
For example, if you defined tools as:

[
  {
    "name": "get_stock_price",
    "description": "Get the current stock price for a given ticker symbol.",
    "input_schema": {
      "type": "object",
      "properties": {
        "ticker": {
          "type": "string",
          "description": "The stock ticker symbol, e.g. AAPL for Apple Inc."
        }
      },
      "required": ["ticker"]
    }
  }
]
And then asked the model "What's the S&P 500 at today?", the model might produce tool_use content blocks in the response like this:

[
  {
    "type": "tool_use",
    "id": "toolu_01D7FLrfh4GYq7yT1ULFeyMV",
    "name": "get_stock_price",
    "input": { "ticker": "^GSPC" }
  }
]
You might then run your get_stock_price tool with {"ticker": "^GSPC"} as an input, and return the following back to the model in a subsequent user message:

[
  {
    "type": "tool_result",
    "tool_use_id": "toolu_01D7FLrfh4GYq7yT1ULFeyMV",
    "content": "259.75 USD"
  }
]
Tools can be used for workflows that include running client-side tools and functions, or more generally whenever you want the model to produce a particular JSON structure of output.

See our guide for more details.

Tool
ComputerUseTool_20241022
BashTool_20241022
TextEditor_20241022

Hide child attributes

tools.type
enum<string> | null
Available options: custom 
tools.description
string
Description of what this tool does.

Tool descriptions should be as detailed as possible. The more information that the model has about what the tool is and how to use it, the better it will perform. You can use natural language descriptions to reinforce important aspects of the tool input JSON schema.

tools.name
string
required
Name of the tool.

This is how the tool will be called by the model and in tool_use blocks.

tools.input_schema
object
required
JSON schema for this tool's input.

This defines the shape of the input that your tool accepts and that the model will produce.


Show child attributes

tools.cache_control
object | null

Show child attributes

top_k
integer
Only sample from the top K options for each subsequent token.

Used to remove "long tail" low probability responses. Learn more technical details here.

Recommended for advanced use cases only. You usually only need to use temperature.

top_p
number
Use nucleus sampling.

In nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by top_p. You should either alter temperature or top_p, but not both.

Recommended for advanced use cases only. You usually only need to use temperature.

Response
200 - application/json
id
string
required
Unique object identifier.

The format and length of IDs may change over time.

type
enum<string>
default: message
required
Object type.

For Messages, this is always "message".

Available options: message 
role
enum<string>
default: assistant
required
Conversational role of the generated message.

This will always be "assistant".

Available options: assistant 
content
object[]
required
Content generated by the model.

This is an array of content blocks, each of which has a type that determines its shape.

Example:

[{"type": "text", "text": "Hi, I'm Claude."}]
If the request input messages ended with an assistant turn, then the response content will continue directly from that last turn. You can use this to constrain the model's output.

For example, if the input messages were:

[
  {"role": "user", "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"},
  {"role": "assistant", "content": "The best answer is ("}
]
Then the response content might be:

[{"type": "text", "text": "B)"}]
Text
Tool Use

Hide child attributes

content.type
enum<string>
default: text
required
Available options: text 
content.text
string
required
model
string
required
The model that handled the request.

stop_reason
enum<string> | null
required
The reason that we stopped.

This may be one the following values:

"end_turn": the model reached a natural stopping point
"max_tokens": we exceeded the requested max_tokens or the model's maximum
"stop_sequence": one of your provided custom stop_sequences was generated
"tool_use": the model invoked one or more tools
In non-streaming mode this value is always non-null. In streaming mode, it is null in the message_start event and non-null otherwise.

Available options: end_turn, max_tokens, stop_sequence, tool_use 
stop_sequence
string | null
required
Which custom stop sequence was generated, if any.

This value will be a non-null string if one of your custom stop sequences was generated.

usage
object
required
Billing and rate-limit usage.

Anthropic's API bills and rate-limits by token counts, as tokens represent the underlying cost to our systems.

Under the hood, the API transforms requests into a format suitable for the model. The model's output then goes through a parsing stage before becoming an API response. As a result, the token counts in usage will not match one-to-one with the exact visible content of an API request or response.

For example, output_tokens will be non-zero, even for an empty string response from Claude.


Hide child attributes

usage.input_tokens
integer
required
The number of input tokens which were used.

usage.cache_creation_input_tokens
integer | null
required
The number of input tokens used to create the cache entry.

usage.cache_read_input_tokens
integer | null
required
The number of input tokens read from the cache.

usage.output_tokens
integer
required
The number of output tokens which were used.



메시지 스트리밍
메시지를 생성할 때 "stream": true를 설정하여 서버 전송 이벤트(SSE)를 사용해 응답을 점진적으로 스트리밍할 수 있습니다.

​
SDK를 사용한 스트리밍
Python과 TypeScript SDK는 여러 가지 스트리밍 방법을 제공합니다. Python SDK는 동기 및 비동기 스트림을 모두 지원합니다. 자세한 내용은 각 SDK의 문서를 참조하세요.


Python

TypeScript

import anthropic

client = anthropic.Anthropic()

with client.messages.stream(
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello"}],
    model="claude-3-5-sonnet-20241022",
) as stream:
  for text in stream.text_stream:
      print(text, end="", flush=True)
​
이벤트 유형
각 서버 전송 이벤트는 명명된 이벤트 유형과 관련 JSON 데이터를 포함합니다. 각 이벤트는 SSE 이벤트 이름(예: event: message_stop)을 사용하고 데이터에 일치하는 이벤트 type을 포함합니다.

각 스트림은 다음과 같은 이벤트 흐름을 사용합니다:

message_start: 빈 content가 있는 Message 객체를 포함합니다.
각각 content_block_start, 하나 이상의 content_block_delta 이벤트, content_block_stop 이벤트가 있는 일련의 콘텐츠 블록. 각 콘텐츠 블록은 최종 Message content 배열의 인덱스에 해당하는 index를 가집니다.
최종 Message 객체의 최상위 변경 사항을 나타내는 하나 이상의 message_delta 이벤트.
최종 message_stop 이벤트.
​
Ping 이벤트
이벤트 스트림에는 여러 개의 ping 이벤트가 포함될 수 있습니다.

​
오류 이벤트
때때로 이벤트 스트림에서 오류가 발생할 수 있습니다. 예를 들어, 사용량이 많은 기간 동안 overloaded_error를 받을 수 있으며, 이는 일반적으로 비스트리밍 상황에서 HTTP 529에 해당합니다:

오류 예시

event: error
data: {"type": "error", "error": {"type": "overloaded_error", "message": "Overloaded"}}
​
기타 이벤트
버전 관리 정책에 따라 새로운 이벤트 유형이 추가될 수 있으며, 코드는 알 수 없는 이벤트 유형을 원활하게 처리해야 합니다.

​
델타 유형
각 content_block_delta 이벤트는 주어진 index에서 content 블록을 업데이트하는 유형의 delta를 포함합니다.

​
텍스트 델타
text 콘텐츠 블록 델타는 다음과 같습니다:

텍스트 델타

event: content_block_delta
data: {"type": "content_block_delta","index": 0,"delta": {"type": "text_delta", "text": "ello frien"}}
​
입력 JSON 델타
tool_use 콘텐츠 블록의 델타는 블록의 input 필드에 대한 업데이트에 해당합니다. 최대 세분성을 지원하기 위해 델타는 _부분 JSON 문자열_인 반면, 최종 tool_use.input은 항상 _객체_입니다.

Pydantic과 같은 라이브러리를 사용하여 부분 JSON을 파싱하거나, 파싱된 증분 값에 접근하기 위한 헬퍼를 제공하는 SDK를 사용하여 문자열 델타를 누적하고 content_block_stop 이벤트를 받으면 JSON을 파싱할 수 있습니다.

tool_use 콘텐츠 블록 델타는 다음과 같습니다:

입력 JSON 델타

event: content_block_delta
data: {"type": "content_block_delta","index": 1,"delta": {"type": "input_json_delta","partial_json": "{\"location\": \"San Fra"}}}
참고: 현재 모델은 input에서 하나의 완전한 키와 값 속성만 방출하는 것을 지원합니다. 따라서 도구를 사용할 때 모델이 작업하는 동안 스트리밍 이벤트 사이에 지연이 있을 수 있습니다. input 키와 값이 누적되면, 향후 모델에서 더 세밀한 세분성을 자동으로 지원할 수 있도록 여러 content_block_delta 이벤트로 청크된 부분 json으로 방출됩니다.

​
Raw HTTP 스트림 응답
스트리밍 모드를 사용할 때는 클라이언트 SDK를 사용하는 것을 강력히 권장합니다. 하지만 직접 API 통합을 구축하는 경우 이러한 이벤트를 직접 처리해야 합니다.

스트림 응답은 다음으로 구성됩니다:

message_start 이벤트
잠재적으로 여러 콘텐츠 블록이 포함되며, 각각은 다음을 포함합니다: a. content_block_start 이벤트 b. 잠재적으로 여러 content_block_delta 이벤트 c. content_block_stop 이벤트
message_delta 이벤트
message_stop 이벤트
응답 전체에 ping 이벤트가 분산되어 있을 수 있습니다. 형식에 대한 자세한 내용은 이벤트 유형을 참조하세요.

​
기본 스트리밍 요청
요청

curl https://api.anthropic.com/v1/messages \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --data \
'{
  "model": "claude-3-5-sonnet-20241022",
  "messages": [{"role": "user", "content": "Hello"}],
  "max_tokens": 256,
  "stream": true
}'
응답

event: message_start
data: {"type": "message_start", "message": {"id": "msg_1nZdL29xx5MUA1yADyHTEsnR8uuvGzszyY", "type": "message", "role": "assistant", "content": [], "model": "claude-3-5-sonnet-20241022", "stop_reason": null, "stop_sequence": null, "usage": {"input_tokens": 25, "output_tokens": 1}}}

event: content_block_start
data: {"type": "content_block_start", "index": 0, "content_block": {"type": "text", "text": ""}}

event: ping
data: {"type": "ping"}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "text_delta", "text": "Hello"}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "text_delta", "text": "!"}}

event: content_block_stop
data: {"type": "content_block_stop", "index": 0}

event: message_delta
data: {"type": "message_delta", "delta": {"stop_reason": "end_turn", "stop_sequence":null}, "usage": {"output_tokens": 15}}

event: message_stop
data: {"type": "message_stop"}
​
도구 사용이 포함된 스트리밍 요청
이 요청에서는 Claude에게 날씨를 알려주는 도구를 사용하도록 요청합니다.

요청

  curl https://api.anthropic.com/v1/messages \
    -H "content-type: application/json" \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -d '{
      "model": "claude-3-5-sonnet-20241022",
      "max_tokens": 1024,
      "tools": [
        {
          "name": "get_weather",
          "description": "Get the current weather in a given location",
          "input_schema": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA"
              }
            },
            "required": ["location"]
          }
        }
      ],
      "tool_choice": {"type": "any"},
      "messages": [
        {
          "role": "user",
          "content": "What is the weather like in San Francisco?"
        }
      ],
      "stream": true
    }'
응답

event: message_start
data: {"type":"message_start","message":{"id":"msg_014p7gG3wDgGV9EUtLvnow3U","type":"message","role":"assistant","model":"claude-3-haiku-20240307","stop_sequence":null,"usage":{"input_tokens":472,"output_tokens":2},"content":[],"stop_reason":null}}

event: content_block_start
data: {"type":"content_block_start","index":0,"content_block":{"type":"text","text":""}}

event: ping
data: {"type": "ping"}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":"Okay"}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":","}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":" let"}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":"'s"}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":" check"}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":" the"}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":" weather"}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":" for"}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":" San"}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":" Francisco"}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":","}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":" CA"}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":":"}}

event: content_block_stop
data: {"type":"content_block_stop","index":0}

event: content_block_start
data: {"type":"content_block_start","index":1,"content_block":{"type":"tool_use","id":"toolu_01T1x1fJ34qAmk2tNTrN7Up6","name":"get_weather","input":{}}}

event: content_block_delta
data: {"type":"content_block_delta","index":1,"delta":{"type":"input_json_delta","partial_json":""}}

event: content_block_delta
data: {"type":"content_block_delta","index":1,"delta":{"type":"input_json_delta","partial_json":"{\"location\":"}}

event: content_block_delta
data: {"type":"content_block_delta","index":1,"delta":{"type":"input_json_delta","partial_json":" \"San"}}

event: content_block_delta
data: {"type":"content_block_delta","index":1,"delta":{"type":"input_json_delta","partial_json":" Francisc"}}

event: content_block_delta
data: {"type":"content_block_delta","index":1,"delta":{"type":"input_json_delta","partial_json":"o,"}}

event: content_block_delta
data: {"type":"content_block_delta","index":1,"delta":{"type":"input_json_delta","partial_json":" CA\""}}

event: content_block_delta
data: {"type":"content_block_delta","index":1,"delta":{"type":"input_json_delta","partial_json":", "}}

event: content_block_delta
data: {"type":"content_block_delta","index":1,"delta":{"type":"input_json_delta","partial_json":"\"unit\": \"fah"}}

event: content_block_delta
data: {"type":"content_block_delta","index":1,"delta":{"type":"input_json_delta","partial_json":"renheit\"}"}}

event: content_block_stop
data: {"type":"content_block_stop","index":1}

event: message_delta
data: {"type":"message_delta","delta":{"stop_reason":"tool_use","stop_sequence":null},"usage":{"output_tokens":89}}

event: message_stop
data: {"type":"message_stop"}




메시지
텍스트 완성에서 마이그레이션하기
텍스트 완성에서 메시지로 마이그레이션하기

텍스트 완성에서 메시지로 마이그레이션할 때 다음과 같은 변경 사항을 고려하세요.

​
입력과 출력
텍스트 완성과 메시지 간의 가장 큰 차이점은 모델 입력을 지정하고 모델로부터 출력을 받는 방식입니다.

텍스트 완성에서는 입력이 원시 문자열입니다:

Python

prompt = "\n\nHuman: Hello there\n\nAssistant: Hi, I'm Claude. How can I help?\n\nHuman: Can you explain Glycolysis to me?\n\nAssistant:"
메시지에서는 원시 프롬프트 대신 입력 메시지 목록을 지정합니다:


Shorthand

Expanded

messages = [
  {"role": "user", "content": "Hello there."},
  {"role": "assistant", "content": "Hi, I'm Claude. How can I help?"},
  {"role": "user", "content": "Can you explain Glycolysis to me?"},
]
각 입력 메시지는 role과 content를 가집니다.

역할 이름

텍스트 완성 API는 \n\nHuman:과 \n\nAssistant:가 번갈아 나타나는 것을 기대하지만, 메시지 API는 user와 assistant 역할을 기대합니다. “human” 또는 “user” 턴을 언급하는 문서를 보실 수 있습니다. 이들은 동일한 역할을 의미하며, 앞으로는 “user”로 통일됩니다.

텍스트 완성에서는 모델이 생성한 텍스트가 응답의 completion 값으로 반환됩니다:

Python

>>> response = anthropic.completions.create(...)
>>> response.completion
" Hi, I'm Claude"
메시지에서는 응답이 콘텐츠 블록 목록인 content 값입니다:

Python

>>> response = anthropic.messages.create(...)
>>> response.content
[{"type": "text", "text": "Hi, I'm Claude"}]
​
Claude의 말을 미리 채우기
텍스트 완성에서는 Claude의 응답 일부를 미리 채울 수 있습니다:

Python

prompt = "\n\nHuman: Hello\n\nAssistant: Hello, my name is"
메시지에서는 마지막 입력 메시지에 assistant 역할을 부여하여 동일한 결과를 얻을 수 있습니다:

Python

messages = [
  {"role": "human", "content": "Hello"},
  {"role": "assistant", "content": "Hello, my name is"},
]
이렇게 하면 응답 content가 마지막 입력 메시지 content에서 이어집니다:

JSON

{
  "role": "assistant",
  "content": [{"type": "text", "text": " Claude. How can I assist you today?" }],
  ...
}
​
시스템 프롬프트
텍스트 완성에서는 첫 번째 \n\nHuman: 턴 이전에 텍스트를 추가하여 시스템 프롬프트를 지정합니다:

Python

prompt = "Today is January 1, 2024.\n\nHuman: Hello, Claude\n\nAssistant:"
메시지에서는 system 매개변수로 시스템 프롬프트를 지정합니다:

Python

anthropic.Anthropic().messages.create(
    model="claude-3-opus-20240229",
    max_tokens=1024,
    system="Today is January 1, 2024.", # <-- 시스템 프롬프트
    messages=[
        {"role": "user", "content": "Hello, Claude"}
    ]
)
​
모델 이름
메시지 API는 전체 모델 버전을 지정해야 합니다(예: claude-3-opus-20240229).

이전에는 주 버전 번호만 지정하는 것을 지원했으며(예: claude-2), 이는 부 버전으로 자동 업그레이드되었습니다. 하지만 이제는 이러한 통합 패턴을 더 이상 권장하지 않으며, 메시지는 이를 지원하지 않습니다.

​
중단 이유
텍스트 완성은 항상 다음 중 하나의 stop_reason을 가집니다:

"stop_sequence": 모델이 자연스럽게 턴을 종료했거나, 사용자가 지정한 중단 시퀀스 중 하나가 생성되었습니다.
"max_tokens": 모델이 지정된 max_tokens만큼의 콘텐츠를 생성했거나, 절대 최대값에 도달했습니다.
메시지는 다음 값 중 하나의 stop_reason을 가집니다:

"end_turn": 대화 턴이 자연스럽게 종료되었습니다.
"stop_sequence": 지정된 사용자 정의 중단 시퀀스 중 하나가 생성되었습니다.
"max_tokens": (변경 없음)
​
최대 토큰 지정
텍스트 완성: max_tokens_to_sample 매개변수. 유효성 검사는 없지만 모델별로 상한값이 있습니다.
메시지: max_tokens 매개변수. 모델이 지원하는 것보다 높은 값을 전달하면 유효성 검사 오류가 반환됩니다.
​
스트리밍 형식
텍스트 완성에서 "stream": true를 사용할 때, 응답에는 completion, ping, error 서버 전송 이벤트가 포함되었습니다. 자세한 내용은 텍스트 완성 스트리밍을 참조하세요.

메시지는 여러 유형의 콘텐츠 블록을 포함할 수 있으므로 스트리밍 형식이 다소 더 복잡합니다. 자세한 내용은 메시지 스트리밍을 참조하세요.





메시지
메시지 예시
메시지 API의 요청 및 응답 예시

전체 매개변수 문서는 API 참조를 참조하세요.

​
기본 요청 및 응답

Shell

Python

TypeScript

import anthropic

message = anthropic.Anthropic().messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "Hello, Claude"}
    ]
)
print(message)
JSON

{
  "id": "msg_01XFDUDYJgAACzvnptvVoYEL",
  "type": "message",
  "role": "assistant",
  "content": [
    {
      "type": "text",
      "text": "Hello!"
    }
  ],
  "model": "claude-3-5-sonnet-20241022",
  "stop_reason": "end_turn",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 12,
    "output_tokens": 6
  }
}
​
다중 대화 턴
메시지 API는 상태를 저장하지 않으므로 항상 전체 대화 기록을 API에 전송해야 합니다. 이 패턴을 사용하여 시간이 지남에 따라 대화를 구축할 수 있습니다. 이전 대화 턴이 반드시 Claude에서 실제로 시작될 필요는 없습니다 - 합성 assistant 메시지를 사용할 수 있습니다.

Shell

#!/bin/sh
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-3-5-sonnet-20241022",
    "max_tokens": 1024,
    "messages": [
        {"role": "user", "content": "Hello, Claude"},
        {"role": "assistant", "content": "Hello!"},
        {"role": "user", "content": "Can you describe LLMs to me?"}
    ]
}'
Python

import anthropic

message = anthropic.Anthropic().messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "Hello, Claude"},
        {"role": "assistant", "content": "Hello!"},
        {"role": "user", "content": "Can you describe LLMs to me?"}
    ],
)
print(message)
TypeScript

import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic();

await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20241022',
  max_tokens: 1024,
  messages: [
    {"role": "user", "content": "Hello, Claude"},
    {"role": "assistant", "content": "Hello!"},
    {"role": "user", "content": "Can you describe LLMs to me?"}
  ]
});
JSON

{
    "id": "msg_018gCsTGsXkYJVqYPxTgDHBU",
    "type": "message",
    "role": "assistant",
    "content": [
        {
            "type": "text",
            "text": "Sure, I'd be happy to provide..."
        }
    ],
    "stop_reason": "end_turn",
    "stop_sequence": null,
    "usage": {
      "input_tokens": 30,
      "output_tokens": 309
    }
}
​
Claude의 답변 미리 채우기
입력 메시지 목록의 마지막 위치에 Claude의 응답 일부를 미리 채울 수 있습니다. 이를 통해 Claude의 응답을 형성할 수 있습니다. 아래 예시는 "max_tokens": 1을 사용하여 Claude로부터 단일 객관식 답변을 얻습니다.


Shell

Python

TypeScript

import anthropic

message = anthropic.Anthropic().messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1,
    messages=[
        {"role": "user", "content": "What is latin for Ant? (A) Apoidea, (B) Rhopalocera, (C) Formicidae"},
        {"role": "assistant", "content": "The answer is ("}
    ]
)
print(message)
JSON

{
  "id": "msg_01Q8Faay6S7QPTvEUUQARt7h",
  "type": "message",
  "role": "assistant",
  "content": [
    {
      "type": "text",
      "text": "C"
    }
  ],
  "model": "claude-3-5-sonnet-20241022",
  "stop_reason": "max_tokens",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 42,
    "output_tokens": 1
  }
}
​
비전
Claude는 요청에서 텍스트와 이미지를 모두 읽을 수 있습니다. 현재 이미지에 대해 base64 소스 유형과 image/jpeg, image/png, image/gif, image/webp 미디어 유형을 지원합니다. 자세한 내용은 비전 가이드를 참조하세요.


Shell

Python

TypeScript

import anthropic
import base64
import httpx

image_url = "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
image_media_type = "image/jpeg"
image_data = base64.standard_b64encode(httpx.get(image_url).content).decode("utf-8")

message = anthropic.Anthropic().messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": image_media_type,
                        "data": image_data,
                    },
                }
            ],
        }
    ],
)
print(message)
JSON

{
  "id": "msg_01EcyWo6m4hyW8KHs2y2pei5",
  "type": "message",
  "role": "assistant",
  "content": [
    {
      "type": "text",
      "text": "This image shows an ant, specifically a close-up view of an ant. The ant is shown in detail, with its distinct head, antennae, and legs clearly visible. The image is focused on capturing the intricate details and features of the ant, likely taken with a macro lens to get an extreme close-up perspective."
    }
  ],
  "model": "claude-3-5-sonnet-20241022",
  "stop_reason": "end_turn",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 1551,
    "output_tokens": 71
  }
}
​
도구 사용, JSON 모드 및 컴퓨터 사용 (베타)
메시지 API에서 도구를 사용하는 방법에 대한 예시는 가이드를 참조하세요. 메시지 API로 데스크톱 컴퓨터 환경을 제어하는 방법에 대한 예시는 컴퓨터 사용 (베타) 가이드를 참조하세요.





메시지 배치 (베타)
메시지 배치 생성하기 (베타)
Send a batch of Message creation requests.

The Message Batches API can be used to process multiple Messages API requests at once. Once a Message Batch is created, it begins processing immediately. Batches can take up to 24 hours to complete.

POST
/
v1
/
messages
/
batches
베타 기간 동안 이 엔드포인트를 사용하려면 anthropic-beta 헤더에 message-batches-2024-09-24 값을 전달해야 합니다
​
기능 지원
메시지 배치 API는 다음 모델들을 지원합니다: Claude 3 Haiku, Claude 3 Opus, 그리고 Claude 3.5 Sonnet. 메시지 API에서 사용 가능한 모든 기능들은 베타 기능을 포함하여 메시지 배치 API를 통해 사용할 수 있습니다.

베타 기간 동안, 배치는 최대 10,000개의 요청을 포함할 수 있으며 총 크기는 32 MB까지 가능합니다.

Headers
anthropic-beta
string[]
Optional header to specify the beta version(s) you want to use.

To use multiple betas, use a comma separated list like beta1,beta2 or specify the header multiple times for each beta.

anthropic-version
string
required
The version of the Anthropic API you want to use.

Read more about versioning and our version history here.

x-api-key
string
required
Your unique API key for authentication.

This key is required in the header of all API requests, to authenticate your account and access Anthropic's services. Get your API key through the Console. Each key is scoped to a Workspace.

Body
application/json
requests
object[]
required
List of requests for prompt completion. Each is an individual request to create a Message.


Show child attributes

Response
200 - application/json
id
string
required
Unique object identifier.

The format and length of IDs may change over time.

type
enum<string>
default: message_batch
required
Object type.

For Message Batches, this is always "message_batch".

Available options: message_batch 
processing_status
enum<string>
required
Processing status of the Message Batch.

Available options: in_progress, canceling, ended 
request_counts
object
required
Tallies requests within the Message Batch, categorized by their status.

Requests start as processing and move to one of the other statuses only once processing of the entire batch ends. The sum of all values always matches the total number of requests in the batch.


Show child attributes

ended_at
string | null
required
RFC 3339 datetime string representing the time at which processing for the Message Batch ended. Specified only once processing ends.

Processing ends when every request in a Message Batch has either succeeded, errored, canceled, or expired.

created_at
string
required
RFC 3339 datetime string representing the time at which the Message Batch was created.

expires_at
string
required
RFC 3339 datetime string representing the time at which the Message Batch will expire and end processing, which is 24 hours after creation.

archived_at
string | null
required
RFC 3339 datetime string representing the time at which the Message Batch was archived and its results became unavailable.

cancel_initiated_at
string | null
required
RFC 3339 datetime string representing the time at which cancellation was initiated for the Message Batch. Specified only if cancellation was initiated.

results_url
string | null
required
URL to a .jsonl file containing the results of the Message Batch requests. Specified only once processing ends.

Results in the file are not guaranteed to be in the same order as requests. Use the custom_id field to match results to requests.




메시지 배치 (베타)
메시지 배치 검색 (베타)
This endpoint is idempotent and can be used to poll for Message Batch completion. To access the results of a Message Batch, make a request to the results_url field in the response.

GET
/
v1
/
messages
/
batches
/
{message_batch_id}
베타 기간 동안 이 엔드포인트를 사용하려면 anthropic-beta 헤더에 message-batches-2024-09-24 값을 전달해야 합니다
Headers
anthropic-beta
string[]
Optional header to specify the beta version(s) you want to use.

To use multiple betas, use a comma separated list like beta1,beta2 or specify the header multiple times for each beta.

anthropic-version
string
required
The version of the Anthropic API you want to use.

Read more about versioning and our version history here.

x-api-key
string
required
Your unique API key for authentication.

This key is required in the header of all API requests, to authenticate your account and access Anthropic's services. Get your API key through the Console. Each key is scoped to a Workspace.

Path Parameters
message_batch_id
string
required
ID of the Message Batch.

Response
200 - application/json
id
string
required
Unique object identifier.

The format and length of IDs may change over time.

type
enum<string>
default: message_batch
required
Object type.

For Message Batches, this is always "message_batch".

Available options: message_batch 
processing_status
enum<string>
required
Processing status of the Message Batch.

Available options: in_progress, canceling, ended 
request_counts
object
required
Tallies requests within the Message Batch, categorized by their status.

Requests start as processing and move to one of the other statuses only once processing of the entire batch ends. The sum of all values always matches the total number of requests in the batch.


Hide child attributes

request_counts.processing
integer
default: 0
required
Number of requests in the Message Batch that are processing.

request_counts.succeeded
integer
default: 0
required
Number of requests in the Message Batch that have completed successfully.

This is zero until processing of the entire Message Batch has ended.

request_counts.errored
integer
default: 0
required
Number of requests in the Message Batch that encountered an error.

This is zero until processing of the entire Message Batch has ended.

request_counts.canceled
integer
default: 0
required
Number of requests in the Message Batch that have been canceled.

This is zero until processing of the entire Message Batch has ended.

request_counts.expired
integer
default: 0
required
Number of requests in the Message Batch that have expired.

This is zero until processing of the entire Message Batch has ended.

ended_at
string | null
required
RFC 3339 datetime string representing the time at which processing for the Message Batch ended. Specified only once processing ends.

Processing ends when every request in a Message Batch has either succeeded, errored, canceled, or expired.

created_at
string
required
RFC 3339 datetime string representing the time at which the Message Batch was created.

expires_at
string
required
RFC 3339 datetime string representing the time at which the Message Batch will expire and end processing, which is 24 hours after creation.

archived_at
string | null
required
RFC 3339 datetime string representing the time at which the Message Batch was archived and its results became unavailable.

cancel_initiated_at
string | null
required
RFC 3339 datetime string representing the time at which cancellation was initiated for the Message Batch. Specified only if cancellation was initiated.

results_url
string | null
required
URL to a .jsonl file containing the results of the Message Batch requests. Specified only once processing ends.

Results in the file are not guaranteed to be in the same order as requests. Use the custom_id field to match results to requests.





메시지 배치 (베타)
메시지 배치 결과 검색 (베타)
Streams the results of a Message Batch as a .jsonl file.

Each line in the file is a JSON object containing the result of a single request in the Message Batch. Results are not guaranteed to be in the same order as requests. Use the custom_id field to match results to requests.

GET
/
v1
/
messages
/
batches
/
{message_batch_id}
/
results
베타 기간 동안 이 엔드포인트는 anthropic-beta 헤더에 message-batches-2024-09-24 값을 전달해야 합니다
메시지 배치 결과를 검색하기 위한 경로는 배치의 results_url에서 가져와야 합니다. 이 경로는 가정되어서는 안 되며 변경될 수 있습니다.
Headers
anthropic-beta
string[]
Optional header to specify the beta version(s) you want to use.

To use multiple betas, use a comma separated list like beta1,beta2 or specify the header multiple times for each beta.

anthropic-version
string
required
The version of the Anthropic API you want to use.

Read more about versioning and our version history here.

x-api-key
string
required
Your unique API key for authentication.

This key is required in the header of all API requests, to authenticate your account and access Anthropic's services. Get your API key through the Console. Each key is scoped to a Workspace.

Path Parameters
message_batch_id
string
required
ID of the Message Batch.

Response
200 - application/x-jsonl
The response is of type file.





메시지 배치 (베타)
메시지 배치 목록 조회 (베타)
List all Message Batches within a Workspace. Most recently created batches are returned first.

GET
/
v1
/
messages
/
batches
베타 기간 동안 이 엔드포인트를 사용하려면 anthropic-beta 헤더에 message-batches-2024-09-24 값을 전달해야 합니다
Headers
anthropic-beta
string[]
Optional header to specify the beta version(s) you want to use.

To use multiple betas, use a comma separated list like beta1,beta2 or specify the header multiple times for each beta.

anthropic-version
string
required
The version of the Anthropic API you want to use.

Read more about versioning and our version history here.

x-api-key
string
required
Your unique API key for authentication.

This key is required in the header of all API requests, to authenticate your account and access Anthropic's services. Get your API key through the Console. Each key is scoped to a Workspace.

Query Parameters
before_id
string
ID of the object to use as a cursor for pagination. When provided, returns the page of results immediately before this object.

after_id
string
ID of the object to use as a cursor for pagination. When provided, returns the page of results immediately after this object.

limit
integer
default: 20
Number of items to return per page.

Defaults to 20. Ranges from 1 to 100.

Response
200 - application/json
data
object[]
required

Hide child attributes

data.id
string
required
Unique object identifier.

The format and length of IDs may change over time.

data.type
enum<string>
default: message_batch
required
Object type.

For Message Batches, this is always "message_batch".

Available options: message_batch 
data.processing_status
enum<string>
required
Processing status of the Message Batch.

Available options: in_progress, canceling, ended 
data.request_counts
object
required
Tallies requests within the Message Batch, categorized by their status.

Requests start as processing and move to one of the other statuses only once processing of the entire batch ends. The sum of all values always matches the total number of requests in the batch.


Show child attributes

data.ended_at
string | null
required
RFC 3339 datetime string representing the time at which processing for the Message Batch ended. Specified only once processing ends.

Processing ends when every request in a Message Batch has either succeeded, errored, canceled, or expired.

data.created_at
string
required
RFC 3339 datetime string representing the time at which the Message Batch was created.

data.expires_at
string
required
RFC 3339 datetime string representing the time at which the Message Batch will expire and end processing, which is 24 hours after creation.

data.archived_at
string | null
required
RFC 3339 datetime string representing the time at which the Message Batch was archived and its results became unavailable.

data.cancel_initiated_at
string | null
required
RFC 3339 datetime string representing the time at which cancellation was initiated for the Message Batch. Specified only if cancellation was initiated.

data.results_url
string | null
required
URL to a .jsonl file containing the results of the Message Batch requests. Specified only once processing ends.

Results in the file are not guaranteed to be in the same order as requests. Use the custom_id field to match results to requests.

has_more
boolean
required
Indicates if there are more results in the requested page direction.

first_id
string | null
required
First ID in the data list. Can be used as the before_id for the previous page.

last_id
string | null
required
Last ID in the data list. Can be used as the after_id for the next page.





메시지 배치 (베타)
메시지 배치 취소하기 (베타)
Batches may be canceled any time before processing ends. Once cancellation is initiated, the batch enters a canceling state, at which time the system may complete any in-progress, non-interruptible requests before finalizing cancellation.

The number of canceled requests is specified in request_counts. To determine which requests were canceled, check the individual results within the batch. Note that cancellation may not result in any canceled requests if they were non-interruptible.

POST
/
v1
/
messages
/
batches
/
{message_batch_id}
/
cancel
베타 기간 동안 이 엔드포인트를 사용하려면 anthropic-beta 헤더에 message-batches-2024-09-24 값을 전달해야 합니다
Headers
anthropic-beta
string[]
Optional header to specify the beta version(s) you want to use.

To use multiple betas, use a comma separated list like beta1,beta2 or specify the header multiple times for each beta.

anthropic-version
string
required
The version of the Anthropic API you want to use.

Read more about versioning and our version history here.

x-api-key
string
required
Your unique API key for authentication.

This key is required in the header of all API requests, to authenticate your account and access Anthropic's services. Get your API key through the Console. Each key is scoped to a Workspace.

Path Parameters
message_batch_id
string
required
ID of the Message Batch.

Response
200 - application/json
id
string
required
Unique object identifier.

The format and length of IDs may change over time.

type
enum<string>
default: message_batch
required
Object type.

For Message Batches, this is always "message_batch".

Available options: message_batch 
processing_status
enum<string>
required
Processing status of the Message Batch.

Available options: in_progress, canceling, ended 
request_counts
object
required
Tallies requests within the Message Batch, categorized by their status.

Requests start as processing and move to one of the other statuses only once processing of the entire batch ends. The sum of all values always matches the total number of requests in the batch.


Hide child attributes

request_counts.processing
integer
default: 0
required
Number of requests in the Message Batch that are processing.

request_counts.succeeded
integer
default: 0
required
Number of requests in the Message Batch that have completed successfully.

This is zero until processing of the entire Message Batch has ended.

request_counts.errored
integer
default: 0
required
Number of requests in the Message Batch that encountered an error.

This is zero until processing of the entire Message Batch has ended.

request_counts.canceled
integer
default: 0
required
Number of requests in the Message Batch that have been canceled.

This is zero until processing of the entire Message Batch has ended.

request_counts.expired
integer
default: 0
required
Number of requests in the Message Batch that have expired.

This is zero until processing of the entire Message Batch has ended.

ended_at
string | null
required
RFC 3339 datetime string representing the time at which processing for the Message Batch ended. Specified only once processing ends.

Processing ends when every request in a Message Batch has either succeeded, errored, canceled, or expired.

created_at
string
required
RFC 3339 datetime string representing the time at which the Message Batch was created.

expires_at
string
required
RFC 3339 datetime string representing the time at which the Message Batch will expire and end processing, which is 24 hours after creation.

archived_at
string | null
required
RFC 3339 datetime string representing the time at which the Message Batch was archived and its results became unavailable.

cancel_initiated_at
string | null
required
RFC 3339 datetime string representing the time at which cancellation was initiated for the Message Batch. Specified only if cancellation was initiated.

results_url
string | null
required
URL to a .jsonl file containing the results of the Message Batch requests. Specified only once processing ends.

Results in the file are not guaranteed to be in the same order as requests. Use the custom_id field to match results to requests.






메시지 배치 (베타)
메시지 배치 예제
메시지 배치 API 사용 예제

메시지 배치 API는 메시지 API와 동일한 기능 세트를 지원합니다. 이 페이지는 메시지 배치 API 사용 방법에 중점을 두고 있으며, 메시지 API 기능 세트의 예제는 메시지 API 예제를 참조하세요.

​
메시지 배치 생성하기

Python

TypeScript

Shell

import anthropic
from anthropic.types.beta.message_create_params import MessageCreateParamsNonStreaming
from anthropic.types.beta.messages.batch_create_params import Request

client = anthropic.Anthropic()

message_batch = client.beta.messages.batches.create(
    requests=[
        Request(
            custom_id="my-first-request",
            params=MessageCreateParamsNonStreaming(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1024,
                messages=[{
                    "role": "user",
                    "content": "Hello, world",
                }]
            )
        ),
        Request(
            custom_id="my-second-request",
            params=MessageCreateParamsNonStreaming(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1024,
                messages=[{
                    "role": "user",
                    "content": "Hi again, friend",
                }]
            )
        )
    ]
)
print(message_batch)
JSON

{
  "id": "msgbatch_013Zva2CMHLNnXjNJJKqJ2EF",
  "type": "message_batch",
  "processing_status": "in_progress",
  "request_counts": {
    "processing": 2,
    "succeeded": 0,
    "errored": 0,
    "canceled": 0,
    "expired": 0
  },
  "ended_at": null,
  "created_at": "2024-09-24T18:37:24.100435Z",
  "expires_at": "2024-09-25T18:37:24.100435Z",
  "cancel_initiated_at": null,
  "results_url": null
}
​
메시지 배치 완료 폴링하기
메시지 배치를 폴링하려면 생성 요청이나 배치 목록에서 제공되는 id가 필요합니다. 예시 id: msgbatch_013Zva2CMHLNnXjNJJKqJ2EF.


Python

TypeScript

Shell

import anthropic

client = anthropic.Anthropic()

message_batch = None
while True:
    message_batch = client.beta.messages.batches.retrieve(
        MESSAGE_BATCH_ID
    )
    if message_batch.processing_status == "ended":
        break
              
    print(f"Batch {MESSAGE_BATCH_ID} is still processing...")
    time.sleep(60)
print(message_batch)
​
워크스페이스의 모든 메시지 배치 나열하기

Python

TypeScript

Shell

import anthropic

client = anthropic.Anthropic()

# 필요에 따라 자동으로 더 많은 페이지를 가져옵니다.
for message_batch in client.beta.messages.batches.list(
    limit=20
):
    print(message_batch)
Output

{
  "id": "msgbatch_013Zva2CMHLNnXjNJJKqJ2EF",
  "type": "message_batch",
  ...
}
{
  "id": "msgbatch_01HkcTjaV5uDC8jWR4ZsDV8d",
  "type": "message_batch",
  ...
}
​
메시지 배치 결과 검색하기
메시지 배치 상태가 ended가 되면 배치의 results_url을 확인하고 .jsonl 파일 형식으로 결과를 검색할 수 있습니다.


Python

TypeScript

Shell

import anthropic

client = anthropic.Anthropic()

# 메모리 효율적인 청크로 결과 파일을 스트리밍하여 한 번에 하나씩 처리
for result in client.beta.messages.batches.results(
    MESSAGE_BATCH_ID,
):
    print(result)
Output

{
  "id": "my-second-request",
  "result": {
    "type": "succeeded",
    "message": {
      "id": "msg_018gCsTGsXkYJVqYPxTgDHBU",
      "type": "message",
      ...
    }
  }
}
{
  "custom_id": "my-first-request",
  "result": {
    "type": "succeeded",
    "message": {
      "id": "msg_01XFDUDYJgAACzvnptvVoYEL",
      "type": "message",
      ...
    }
  }
}
​
메시지 배치 취소하기
취소 직후에는 배치의 processing_status가 canceling이 됩니다. 취소된 배치도 결국 ended 상태가 되고 결과를 포함할 수 있으므로, 동일한 배치 완료 폴링 기술을 사용하여 취소가 완료되었는지 폴링할 수 있습니다.


Python

TypeScript

Shell

import anthropic

client = anthropic.Anthropic()

message_batch = client.beta.messages.batches.cancel(
    MESSAGE_BATCH_ID,
)
print(message_batch)
JSON

{
  "id": "msgbatch_013Zva2CMHLNnXjNJJKqJ2EF",
  "type": "message_batch",
  "processing_status": "canceling",
  "request_counts": {
    "processing": 2,
    "succeeded": 0,
    "errored": 0,
    "canceled": 0,
    "expired": 0
  },
  "ended_at": null,
  "created_at": "2024-09-24T18:37:24.100435Z",
  "expires_at": "2024-09-25T18:37:24.100435Z",
  "cancel_initiated_at": "2024-09-24T18:39:03.114875Z",
  "results_url": null
}